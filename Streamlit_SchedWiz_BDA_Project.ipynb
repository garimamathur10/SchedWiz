{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SchedWiz: AI-Powered Study Scheduler**\n"
      ],
      "metadata": {
        "id": "AsVwk4g8EwGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "WznlQISBE2YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from datetime import date, timedelta\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Configuring PySpark to run with Streamlit\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n"
      ],
      "metadata": {
        "id": "X-rphs9-E51Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrok Authentication Token"
      ],
      "metadata": {
        "id": "hdxrbXcHF2Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2xoDBn5Ii9vasE8s0PSuJe2tj7l_31yAMercJ42t26zSGqc7P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Be-NwMF17f",
        "outputId": "bef796e1-6b99-4cf5-d206-e115eeb13679"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Classifier Model"
      ],
      "metadata": {
        "id": "Peij2pQrE9zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_agent.py\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer\n",
        "from pyspark.ml import Pipeline\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# Neural network Module\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# Data Loading and Feature Engineering\n",
        "def prepare_student_dataset():\n",
        "    info = pd.read_csv(\"/content/drive/MyDrive/open+university+learning+analytics+dataset/studentInfo.csv\")\n",
        "    assessment = pd.read_csv(\"/content/drive/MyDrive/open+university+learning+analytics+dataset/studentAssessment.csv\")\n",
        "    meta = pd.read_csv(\"/content/drive/MyDrive/open+university+learning+analytics+dataset/assessments.csv\")\n",
        "    vle = pd.read_csv(\"/content/drive/MyDrive/open+university+learning+analytics+dataset/studentVle.csv\")\n",
        "\n",
        "    assessment = assessment.merge(meta[['id_assessment', 'code_module', 'date', 'weight']], on='id_assessment')\n",
        "    assessment['score'] = pd.to_numeric(assessment['score'], errors='coerce')\n",
        "\n",
        "    def weighted(x):\n",
        "        w = assessment.loc[x.index, 'weight'].fillna(0)\n",
        "        return np.average(x.fillna(0), weights=w) if w.sum() > 0 else x.mean()\n",
        "\n",
        "    agg = assessment.groupby(['id_student', 'code_module']).agg(\n",
        "        avg_score=('score', 'mean'),\n",
        "        std_score=('score', 'std'),\n",
        "        count=('score', 'count'),\n",
        "        last_score=('score', lambda x: x.iloc[-1]),\n",
        "        weighted_score=('score', weighted)\n",
        "    ).reset_index()\n",
        "\n",
        "    trend = assessment.sort_values(['id_student', 'date']).groupby('id_student')['score'].apply(\n",
        "        lambda x: np.polyfit(range(len(x)), x.fillna(0), 1)[0] if len(x) > 1 else 0).reset_index(name='score_trend')\n",
        "\n",
        "    vle_feat = vle.groupby('id_student').agg(\n",
        "        total_clicks=('sum_click', 'sum'),\n",
        "        active_days=('date', 'nunique'),\n",
        "        avg_clicks_per_day=('sum_click', lambda x: x.sum()/len(x)),\n",
        "        click_std=('sum_click', 'std')).reset_index()\n",
        "\n",
        "    f14 = vle[vle['date'] <= 14].groupby('id_student')['sum_click'].sum().reset_index(name='clicks_first_14_days')\n",
        "    l7 = vle[vle['date'] >= vle['date'].max()-7].groupby('id_student')['sum_click'].sum().reset_index(name='clicks_last_7_days')\n",
        "\n",
        "    demo = info[['id_student', 'code_module', 'final_result', 'age_band', 'highest_education', 'imd_band']]\n",
        "    demo[['age_band', 'highest_education', 'imd_band']] = OrdinalEncoder().fit_transform(\n",
        "        demo[['age_band', 'highest_education', 'imd_band']].astype(str)\n",
        "    )\n",
        "\n",
        "    df = agg.merge(demo, on=['id_student', 'code_module'])\n",
        "    df = df.merge(vle_feat, on='id_student')\n",
        "    df = df.merge(trend, on='id_student')\n",
        "    df = df.merge(f14, on='id_student')\n",
        "    df = df.merge(l7, on='id_student')\n",
        "    df = df[df['final_result'] != 'Withdrawn']\n",
        "    df = df[df['count'] >= 2]\n",
        "\n",
        "    df['target_class'] = df['final_result'].map({'Fail': 0, 'Pass': 1, 'Distinction': 2})\n",
        "    df['score_click_interaction'] = df['avg_score'] * df['total_clicks']\n",
        "    df['click_std_ratio'] = df['click_std'] / (df['avg_clicks_per_day'] + 1e-3)\n",
        "\n",
        "    features = ['avg_score', 'std_score', 'count', 'last_score', 'score_trend', 'weighted_score',\n",
        "                'total_clicks', 'active_days', 'avg_clicks_per_day', 'click_std',\n",
        "                'clicks_first_14_days', 'clicks_last_7_days',\n",
        "                'age_band', 'highest_education', 'imd_band',\n",
        "                'score_click_interaction', 'click_std_ratio']\n",
        "\n",
        "    return df, features\n",
        "\n",
        "# Training Pipeline\n",
        "spark = SparkSession.builder.appName(\"StudentNN\").getOrCreate()\n",
        "data_df, features = prepare_student_dataset()\n",
        "sdf = spark.createDataFrame(data_df.dropna(subset=['target_class']))\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    Imputer(inputCols=features, outputCols=features),\n",
        "    VectorAssembler(inputCols=features, outputCol=\"features_vec\"),\n",
        "    StandardScaler(inputCol=\"features_vec\", outputCol=\"features\")\n",
        "])\n",
        "\n",
        "fitted_pipeline = pipeline.fit(sdf)\n",
        "final_df = fitted_pipeline.transform(sdf).select(\"features\", \"target_class\")\n",
        "\n",
        "X = np.array(final_df.select(\"features\").rdd.map(lambda x: x[0].toArray()).collect())\n",
        "y = np.array(final_df.select(\"target_class\").rdd.map(lambda x: int(x[0])).collect())\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "model = MLP(X.shape[1], 3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for _ in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(X_tensor)\n",
        "    loss = loss_fn(out, y_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "feature_means = dict(zip(features, data_df[features].mean().to_dict()))\n",
        "\n",
        "# Scheduler Agent\n",
        "class SchedulerAgent:\n",
        "    def __init__(self, predicted_scores, exam_dates, total_daily_hours, subject_inputs):\n",
        "        self.predicted_scores = predicted_scores\n",
        "        self.exam_dates = exam_dates\n",
        "        self.total_daily_hours = total_daily_hours\n",
        "        self.subject_inputs = subject_inputs\n",
        "\n",
        "    def _priority(self):\n",
        "        scores = {}\n",
        "        for subj, pred_class in self.predicted_scores.items():\n",
        "            diff = self.subject_inputs[subj].get(\"difficulty_level\", 2)\n",
        "            score = self.subject_inputs[subj].get(\"last_score\", 60)\n",
        "            raw = (2 - pred_class) * 3 + diff * 2 + (100 - score) / 20\n",
        "            scores[subj] = raw\n",
        "        vals = list(scores.values())\n",
        "        min_val, max_val = min(vals), max(vals)\n",
        "        return {s: round(1 + 9 * ((v - min_val) / (max_val - min_val + 1e-5)), 2) for s, v in scores.items()}\n",
        "\n",
        "    def _days_left(self):\n",
        "        today = date.today()\n",
        "        return {s: max(1, (date.fromisoformat(d) - today).days) for s, d in self.exam_dates.items()}\n",
        "\n",
        "    def run(self):\n",
        "        p_scores = self._priority()\n",
        "        d_left = self._days_left()\n",
        "        daily_hours_map = {}\n",
        "\n",
        "        for s in p_scores:\n",
        "            if d_left[s] == 1:\n",
        "                daily_hours_map[s] = self.total_daily_hours\n",
        "            else:\n",
        "                daily_hours_map[s] = round((p_scores[s] / sum(p_scores.values())) * self.total_daily_hours * d_left[s], 2)\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            \"Subject\": list(self.predicted_scores.keys()),\n",
        "            \"Predicted Class\": [self.predicted_scores[s] for s in self.predicted_scores],\n",
        "            \"Priority Score\": [p_scores[s] for s in self.predicted_scores],\n",
        "            \"Days Until Exam\": [d_left[s] for s in self.predicted_scores],\n",
        "            \"Total Hours Assigned\": [daily_hours_map[s] for s in self.predicted_scores]\n",
        "        }).sort_values(by=\"Priority Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    def generate_daily_plan(self):\n",
        "        summary = self.run()\n",
        "        today = date.today()\n",
        "        plan = []\n",
        "\n",
        "        for i in range(max(summary['Days Until Exam'])):\n",
        "            current_day = (today + timedelta(days=i)).isoformat()\n",
        "            valid_subjects = summary[summary['Days Until Exam'] > i]\n",
        "            weights = valid_subjects[\"Priority Score\"].tolist()\n",
        "            total_priority = sum(weights)\n",
        "\n",
        "            for _, row in valid_subjects.iterrows():\n",
        "                if row['Days Until Exam'] == 1:\n",
        "                    hrs = self.total_daily_hours\n",
        "                else:\n",
        "                    hrs = round((row[\"Priority Score\"] / total_priority) * self.total_daily_hours, 2)\n",
        "                plan.append({\"Date\": current_day, \"Subject\": row[\"Subject\"], \"Hours\": hrs})\n",
        "\n",
        "        return pd.DataFrame(plan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpWRxjwqFLMh",
        "outputId": "7b46b97b-4ab7-4983-89c5-76bd2601ae96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit Dashboard App"
      ],
      "metadata": {
        "id": "S4i40djUG1Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import date\n",
        "from train_agent import model, fitted_pipeline, SchedulerAgent, features, feature_means\n",
        "from pyspark.sql import SparkSession\n",
        "import plotly.express as px\n",
        "\n",
        "# Helper Function to predict outcome for subject\n",
        "def predict(model, pipeline, subject_input_dict):\n",
        "    preds = {}\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    for code, feats in subject_input_dict.items():\n",
        "        filled = {k: feats.get(k, feature_means[k]) for k in features}\n",
        "        pdf = pd.DataFrame([filled])\n",
        "        sdf = spark.createDataFrame(pdf)\n",
        "        transformed = pipeline.transform(sdf).select(\"features\")\n",
        "        X = np.array(transformed.rdd.map(lambda x: x[0].toArray()).collect())\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            logits = model(X_tensor)\n",
        "            pred = torch.argmax(logits, axis=1).item()\n",
        "        preds[code] = pred\n",
        "    return preds\n",
        "\n",
        "# Main Dashboard Function\n",
        "def main():\n",
        "    st.set_page_config(layout=\"wide\")\n",
        "    st.title(\"🧙‍♂️ SchedWiz: Your AI Study Wizard\")\n",
        "    st.caption(\"Turn your stress into strategy — Powered by PyTorch & Spark ✨\")\n",
        "\n",
        "    # Subject selection\n",
        "    subject_codes = ['AAA', 'BBB', 'CCC', 'DDD', 'EEE', 'FFF', 'GGG']\n",
        "    subject_map = {\n",
        "        'AAA': 'Social Sciences', 'BBB': 'Business Analytics', 'CCC': 'Computer Science',\n",
        "        'DDD': 'Mathematics', 'EEE': 'Engineering Design', 'FFF': 'AI Ethics', 'GGG': 'Data Structures'\n",
        "    }\n",
        "\n",
        "    st.markdown(\"### 📝 Select Your Subjects\")\n",
        "    selected = st.multiselect(\"Choose your subjects (max 7)\", options=subject_codes, format_func=lambda x: subject_map[x])\n",
        "\n",
        "    exams, inputs = {}, {}\n",
        "    for s in selected:\n",
        "        with st.expander(f\"📘 {subject_map.get(s, s)}\", expanded=False):\n",
        "            exams[s] = st.date_input(f\"🗓️ When is your exam for {subject_map.get(s, s)}?\", value=date.today(), key=f\"date_{s}\").isoformat()\n",
        "            diff = st.slider(f\"📊 How hard is {subject_map.get(s, s)}?\", 1, 3, 2, help=\"1 = Easy, 3 = Hard\", key=f\"diff_{s}\")\n",
        "            score = st.slider(f\"📈 What was your last score in {subject_map.get(s, s)}?\", 0, 100, 60, help=\"Used to estimate risk of failing\", key=f\"score_{s}\")\n",
        "            d = {k: 0.0 for k in features}\n",
        "            d[\"difficulty_level\"] = diff\n",
        "            d[\"last_score\"] = score\n",
        "            inputs[s] = d\n",
        "\n",
        "    hrs = st.slider(\"⏳ How many hours can you study per day?\", 1, 12, 4)\n",
        "\n",
        "    # Generating Plan On Click\n",
        "    if st.button(\"✨ Generate My Smart Schedule\"):\n",
        "        try:\n",
        "            preds = predict(model, fitted_pipeline, inputs)\n",
        "            agent = SchedulerAgent(preds, exams, hrs, inputs)\n",
        "            plan_df = agent.run()\n",
        "            plan_df['Subject'] = plan_df['Subject'].map(subject_map).fillna(plan_df['Subject'])\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "            st.markdown(\"### 📋 What This Means\")\n",
        "            st.info(\"\"\"\n",
        "- **Predicted Class**:\n",
        "  - 0 = ❌ Fail (score < 30)\n",
        "  - 1 = ✅ Pass (30–79)\n",
        "  - 2 = 🌟 Distinction (80+)\n",
        "\n",
        "- **Priority Score**: Combines AI risk + difficulty + past score, scaled 1–10\n",
        "\"\"\")\n",
        "\n",
        "            # Creating a plan summary table\n",
        "            st.markdown(\"#### 🧠 Smart Summary of Your Subjects\")\n",
        "            st.dataframe(\n",
        "                plan_df.style\n",
        "                    .bar(subset=[\"Total Hours Assigned\"], color='#FFA07A')\n",
        "                    .format({\n",
        "                        \"Total Hours Assigned\": \"{:.2f}\",\n",
        "                        \"Priority Score\": \"{:.2f}\"\n",
        "                    }),\n",
        "                use_container_width=True\n",
        "            )\n",
        "\n",
        "            # Bubble Chart\n",
        "            st.markdown(\"#### 🎯 Visual Allocation of Study Time\")\n",
        "            fig = px.scatter(plan_df, x=\"Subject\", y=\"Total Hours Assigned\", color=\"Priority Score\",\n",
        "                             size=\"Priority Score\", title=\"Lollipop-Style Study Allocation\", size_max=20)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            # Daily Plan\n",
        "            st.markdown(\"#### 📅 Daily Wizard Plan\")\n",
        "            daily = agent.generate_daily_plan()\n",
        "            daily['Subject'] = daily['Subject'].map(subject_map).fillna(daily['Subject'])\n",
        "            styled = daily.style.background_gradient(subset=[\"Hours\"], cmap=\"BuGn\").format({\"Hours\": \"{:.0f}\"})\n",
        "            st.dataframe(styled, use_container_width=True)\n",
        "\n",
        "            # Downlopad Buttons\n",
        "            st.download_button(\"⬇️ Download Full Schedule\", plan_df.to_csv(index=False), file_name=\"schedule.csv\")\n",
        "            st.download_button(\"⬇️ Download Daily Plan\", daily.to_csv(index=False), file_name=\"daily_schedule.csv\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Oops! Something went wrong: {e}\")\n",
        "\n",
        "# Entry Point\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fye0k9O_GImh",
        "outputId": "f8c03990-9552-4824-970d-f3c1ea030eb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit + NGROK Lanch cell for colab"
      ],
      "metadata": {
        "id": "m3vGMGgIHer1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "def run_streamlit():\n",
        "    !streamlit run app.py &> /dev/null\n",
        "\n",
        "# Starting Streamlit in a background thread\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Waiting briefly for Streamlit to boot\n",
        "time.sleep(5)\n",
        "\n",
        "# Opening a tunnel to port 8501\n",
        "public_url = ngrok.connect(addr=\"http://localhost:8501\", proto=\"http\")\n",
        "print(f\"Your SchedWiz app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-azpqHlsGRts",
        "outputId": "f56efd21-1452-4293-92b3-e9b40d7c8124"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your SchedWiz app is live at: NgrokTunnel: \"https://0a97-34-82-54-174.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}